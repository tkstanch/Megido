"""
Advanced Analysis Engine for Malware Analyzer

This module provides extra extremely advanced capabilities including:
- PE file parsing and analysis
- YARA rule scanning
- Threat intelligence integration
- ML-based detection
- Advanced visualization data
"""

import hashlib
import logging
import re
from typing import Dict, List, Tuple, Optional
from collections import Counter
import math

logger = logging.getLogger(__name__)


class PEAnalyzer:
    """
    Advanced PE (Portable Executable) file analyzer.
    
    Provides detailed analysis of Windows PE files including:
    - PE structure parsing
    - Section analysis
    - Import/Export tables
    - Resource extraction
    - Digital signature validation
    - Anomaly detection
    """
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.file_data = None
        self.pe_available = False
        
        try:
            import pefile
            self.pefile = pefile
            self.pe_available = True
        except ImportError:
            logger.warning("pefile library not available, using basic analysis")
            self.pe_available = False
    
    def analyze(self) -> Dict:
        """Perform comprehensive PE analysis."""
        result = {
            'is_pe': False,
            'pe_type': None,
            'sections': [],
            'imports': [],
            'exports': [],
            'resources': [],
            'certificates': [],
            'anomalies': [],
            'characteristics': {},
            'entropy_per_section': {},
        }
        
        try:
            with open(self.file_path, 'rb') as f:
                self.file_data = f.read()
            
            # Check if it's a PE file
            if not self._is_pe_file():
                return result
            
            result['is_pe'] = True
            
            if self.pe_available:
                result.update(self._parse_with_pefile())
            else:
                result.update(self._parse_basic())
        
        except Exception as e:
            logger.error(f"Error analyzing PE file: {e}")
            result['error'] = str(e)
        
        return result
    
    def _is_pe_file(self) -> bool:
        """Check if file has PE signature."""
        if len(self.file_data) < 64:
            return False
        
        # Check DOS signature 'MZ'
        if self.file_data[0:2] != b'MZ':
            return False
        
        # Get PE offset
        pe_offset = int.from_bytes(self.file_data[60:64], byteorder='little')
        
        if pe_offset >= len(self.file_data) - 4:
            return False
        
        # Check PE signature 'PE\0\0'
        if self.file_data[pe_offset:pe_offset+4] != b'PE\0\0':
            return False
        
        return True
    
    def _parse_with_pefile(self) -> Dict:
        """Parse PE using pefile library."""
        result = {}
        
        try:
            pe = self.pefile.PE(data=self.file_data, fast_load=True)
            
            # Parse full PE structure
            pe.parse_data_directories()
            
            # PE type
            if pe.FILE_HEADER.Machine == 0x14c:
                result['pe_type'] = 'PE32 (32-bit)'
            elif pe.FILE_HEADER.Machine == 0x8664:
                result['pe_type'] = 'PE32+ (64-bit)'
            else:
                result['pe_type'] = f'Unknown (0x{pe.FILE_HEADER.Machine:x})'
            
            # Characteristics
            result['characteristics'] = {
                'timestamp': pe.FILE_HEADER.TimeDateStamp,
                'image_base': hex(pe.OPTIONAL_HEADER.ImageBase),
                'entry_point': hex(pe.OPTIONAL_HEADER.AddressOfEntryPoint),
                'subsystem': pe.OPTIONAL_HEADER.Subsystem,
            }
            
            # Sections
            result['sections'] = []
            for section in pe.sections:
                section_data = {
                    'name': section.Name.decode('utf-8', errors='ignore').strip('\x00'),
                    'virtual_address': hex(section.VirtualAddress),
                    'virtual_size': section.Misc_VirtualSize,
                    'raw_size': section.SizeOfRawData,
                    'entropy': section.get_entropy(),
                    'characteristics': hex(section.Characteristics),
                }
                result['sections'].append(section_data)
                
                # Check for anomalies
                if section.SizeOfRawData == 0 and section.Misc_VirtualSize > 0:
                    result['anomalies'].append(f"Section {section_data['name']} has no raw data but has virtual size")
                
                if section.get_entropy() > 7.0:
                    result['anomalies'].append(f"Section {section_data['name']} has high entropy ({section.get_entropy():.2f}), possibly packed/encrypted")
            
            # Imports
            if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                for entry in pe.DIRECTORY_ENTRY_IMPORT[:20]:  # Limit to first 20
                    dll_name = entry.dll.decode('utf-8', errors='ignore')
                    imports = [imp.name.decode('utf-8', errors='ignore') if imp.name else f"Ordinal_{imp.ordinal}" 
                              for imp in entry.imports[:10]]  # Limit to 10 imports per DLL
                    result['imports'].append({
                        'dll': dll_name,
                        'functions': imports
                    })
            
            # Exports
            if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
                result['exports'] = []
                for exp in pe.DIRECTORY_ENTRY_EXPORT.symbols[:50]:  # Limit to 50
                    if exp.name:
                        result['exports'].append({
                            'name': exp.name.decode('utf-8', errors='ignore'),
                            'ordinal': exp.ordinal,
                            'address': hex(exp.address)
                        })
            
            # Resources
            if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
                result['resources'] = self._extract_resources(pe)
            
            # Digital signatures
            if hasattr(pe, 'DIRECTORY_ENTRY_SECURITY'):
                result['has_signature'] = True
                result['certificates'] = ['Digital signature present']
            else:
                result['has_signature'] = False
            
            # Detect suspicious characteristics
            self._detect_pe_anomalies(pe, result)
            
            pe.close()
            
        except Exception as e:
            logger.error(f"Error parsing PE with pefile: {e}")
            result['parse_error'] = str(e)
        
        return result
    
    def _parse_basic(self) -> Dict:
        """Basic PE parsing without pefile library."""
        result = {
            'pe_type': 'PE (basic detection)',
            'note': 'Install pefile library for detailed analysis'
        }
        
        try:
            # Get PE offset
            pe_offset = int.from_bytes(self.file_data[60:64], byteorder='little')
            
            # Read machine type
            machine = int.from_bytes(self.file_data[pe_offset+4:pe_offset+6], byteorder='little')
            
            if machine == 0x14c:
                result['pe_type'] = 'PE32 (32-bit) - basic detection'
            elif machine == 0x8664:
                result['pe_type'] = 'PE32+ (64-bit) - basic detection'
            
        except Exception as e:
            logger.error(f"Error in basic PE parsing: {e}")
        
        return result
    
    def _extract_resources(self, pe) -> List[Dict]:
        """Extract resource information."""
        resources = []
        try:
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(resource_type, 'directory'):
                    for resource_id in resource_type.directory.entries:
                        if hasattr(resource_id, 'directory'):
                            for resource_lang in resource_id.directory.entries:
                                data_rva = resource_lang.data.struct.OffsetToData
                                size = resource_lang.data.struct.Size
                                resources.append({
                                    'type': resource_type.struct.name if hasattr(resource_type.struct, 'name') else resource_type.struct.Id,
                                    'size': size,
                                    'rva': hex(data_rva)
                                })
                                if len(resources) >= 20:  # Limit
                                    return resources
        except Exception as e:
            logger.error(f"Error extracting resources: {e}")
        
        return resources
    
    def _detect_pe_anomalies(self, pe, result: Dict):
        """Detect suspicious PE characteristics."""
        anomalies = result.get('anomalies', [])
        
        # Check for suspicious entry point
        if hasattr(pe, 'OPTIONAL_HEADER'):
            ep = pe.OPTIONAL_HEADER.AddressOfEntryPoint
            ep_section = pe.get_section_by_rva(ep)
            if ep_section:
                ep_section_name = ep_section.Name.decode('utf-8', errors='ignore').strip('\x00')
                if ep_section_name not in ['.text', 'CODE']:
                    anomalies.append(f"Unusual entry point section: {ep_section_name}")
        
        # Check timestamp
        if hasattr(pe, 'FILE_HEADER'):
            timestamp = pe.FILE_HEADER.TimeDateStamp
            if timestamp == 0:
                anomalies.append("Timestamp is zero (possibly tampered)")
        
        # Check for suspicious imports
        suspicious_imports = ['CreateRemoteThread', 'WriteProcessMemory', 'VirtualAllocEx', 
                            'SetWindowsHookEx', 'GetAsyncKeyState', 'InternetReadFile']
        
        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
            found_suspicious = []
            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                for imp in entry.imports:
                    if imp.name and imp.name.decode('utf-8', errors='ignore') in suspicious_imports:
                        found_suspicious.append(imp.name.decode('utf-8', errors='ignore'))
            
            if found_suspicious:
                anomalies.append(f"Suspicious imports detected: {', '.join(found_suspicious[:5])}")
        
        result['anomalies'] = anomalies


class YARAScanner:
    """
    YARA rule scanner for malware detection.
    
    Scans files against YARA rules to detect known malware patterns.
    """
    
    def __init__(self):
        self.yara_available = False
        try:
            import yara
            self.yara = yara
            self.yara_available = True
        except ImportError:
            logger.warning("yara-python library not available")
    
    def scan_file(self, file_path: str, rules_path: Optional[str] = None) -> Dict:
        """
        Scan file with YARA rules.
        
        Args:
            file_path: Path to file to scan
            rules_path: Optional path to YARA rules file
        
        Returns:
            Dictionary with scan results
        """
        result = {
            'matches': [],
            'scanned': False,
            'error': None
        }
        
        if not self.yara_available:
            result['error'] = 'YARA library not available'
            return result
        
        try:
            # Use default rules if none provided
            if rules_path:
                rules = self.yara.compile(filepath=rules_path)
            else:
                # Create basic default rules
                rules = self._get_default_rules()
            
            matches = rules.match(file_path)
            
            result['scanned'] = True
            result['matches'] = [
                {
                    'rule': match.rule,
                    'namespace': match.namespace,
                    'tags': match.tags,
                    'meta': match.meta,
                    'strings': [(s[0], s[1], s[2].decode('utf-8', errors='ignore')[:100]) 
                               for s in match.strings[:10]]  # Limit string matches
                }
                for match in matches
            ]
            
        except Exception as e:
            logger.error(f"Error scanning with YARA: {e}")
            result['error'] = str(e)
        
        return result
    
    def _get_default_rules(self):
        """Get default YARA rules for common malware patterns."""
        default_rules = '''
        rule Suspicious_PE_Characteristics {
            meta:
                description = "Detects suspicious PE characteristics"
                severity = "medium"
            strings:
                $mz = "MZ"
                $suspicious1 = "cmd.exe" nocase
                $suspicious2 = "powershell" nocase
                $suspicious3 = "CreateRemoteThread"
                $suspicious4 = "VirtualAllocEx"
            condition:
                $mz at 0 and (2 of ($suspicious*))
        }
        
        rule Ransomware_Indicators {
            meta:
                description = "Detects potential ransomware indicators"
                severity = "high"
            strings:
                $ransom1 = "bitcoin" nocase
                $ransom2 = "decrypt" nocase
                $ransom3 = "ransom" nocase
                $ransom4 = ".onion" nocase
                $crypto1 = "AES" nocase
                $crypto2 = "RSA" nocase
            condition:
                (2 of ($ransom*)) and (1 of ($crypto*))
        }
        
        rule Keylogger_Indicators {
            meta:
                description = "Detects potential keylogger"
                severity = "high"
            strings:
                $api1 = "GetAsyncKeyState"
                $api2 = "GetKeyState"
                $api3 = "SetWindowsHookEx"
                $log = "keylog" nocase
            condition:
                2 of them
        }
        
        rule Network_Activity {
            meta:
                description = "Detects network activity indicators"
                severity = "medium"
            strings:
                $url1 = "http://" nocase
                $url2 = "https://" nocase
                $api1 = "InternetOpenUrl"
                $api2 = "InternetReadFile"
                $api3 = "URLDownloadToFile"
            condition:
                (1 of ($url*)) and (1 of ($api*))
        }
        '''
        
        return self.yara.compile(source=default_rules)


class ThreatIntelligence:
    """
    Threat intelligence integration for malware analysis.
    
    Provides hash lookups, reputation checks, and IOC enrichment.
    """
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self.vt_available = api_key is not None
    
    def lookup_hash(self, file_hash: str) -> Dict:
        """
        Look up file hash in threat intelligence databases.
        
        Args:
            file_hash: SHA256 hash of file
        
        Returns:
            Dictionary with threat intelligence results
        """
        result = {
            'hash': file_hash,
            'known_malware': False,
            'detection_rate': None,
            'names': [],
            'first_seen': None,
            'last_seen': None,
            'sources': []
        }
        
        # VirusTotal lookup (if API key available)
        if self.vt_available:
            vt_result = self._virustotal_lookup(file_hash)
            if vt_result:
                result.update(vt_result)
        else:
            result['note'] = 'Configure VirusTotal API key for threat intelligence lookups'
        
        return result
    
    def _virustotal_lookup(self, file_hash: str) -> Optional[Dict]:
        """Look up hash on VirusTotal."""
        # Stub implementation - requires actual API integration
        logger.info(f"VirusTotal lookup for hash: {file_hash}")
        return {
            'vt_available': True,
            'note': 'VirusTotal API integration ready (configure API key in settings)'
        }
    
    def enrich_iocs(self, iocs: Dict) -> Dict:
        """
        Enrich Indicators of Compromise with threat intelligence.
        
        Args:
            iocs: Dictionary of IOCs (hashes, IPs, domains, URLs)
        
        Returns:
            Enriched IOC data
        """
        enriched = {}
        
        # Enrich file hashes
        if 'hashes' in iocs:
            enriched['hashes'] = {}
            for hash_type, hash_value in iocs['hashes'].items():
                if hash_type == 'sha256':
                    enriched['hashes'][hash_type] = self.lookup_hash(hash_value)
                else:
                    enriched['hashes'][hash_type] = {'value': hash_value}
        
        # Enrich IPs (stub)
        if 'ips' in iocs:
            enriched['ips'] = [{'ip': ip, 'reputation': 'Unknown'} for ip in iocs['ips']]
        
        # Enrich domains (stub)
        if 'domains' in iocs:
            enriched['domains'] = [{'domain': d, 'reputation': 'Unknown'} for d in iocs['domains']]
        
        return enriched


class MLDetector:
    """
    Machine Learning-based malware detector.
    
    Uses file characteristics for classification.
    """
    
    def __init__(self):
        self.model_available = False
        self.features = []
    
    def extract_features(self, file_path: str) -> List[float]:
        """Extract features for ML classification."""
        features = []
        
        try:
            with open(file_path, 'rb') as f:
                data = f.read()
            
            # File size
            features.append(len(data))
            
            # Entropy
            if len(data) > 0:
                counter = Counter(data)
                entropy = 0
                for count in counter.values():
                    p = count / len(data)
                    if p > 0:
                        entropy -= p * math.log2(p)
                features.append(entropy)
            else:
                features.append(0)
            
            # Byte frequency distribution (first 10 bytes)
            byte_freq = [0] * 10
            for i in range(min(10, 256)):
                byte_freq[i % 10] += data.count(i.to_bytes(1, 'big'))
            features.extend(byte_freq[:10])
            
            # String count
            strings = re.findall(rb'[\x20-\x7e]{4,}', data[:10000])
            features.append(len(strings))
            
            # URL count
            urls = re.findall(rb'https?://', data[:10000])
            features.append(len(urls))
            
        except Exception as e:
            logger.error(f"Error extracting features: {e}")
            features = [0] * 14
        
        return features
    
    def predict(self, features: List[float]) -> Dict:
        """
        Predict if file is malicious.
        
        Args:
            features: Extracted features
        
        Returns:
            Prediction result with confidence
        """
        result = {
            'is_malicious': False,
            'confidence': 0.0,
            'model_used': 'heuristic',
            'explanation': []
        }
        
        # Simple heuristic-based prediction (no ML model yet)
        score = 0
        
        # High entropy suggests packing
        if len(features) > 1 and features[1] > 7.0:
            score += 0.3
            result['explanation'].append('High entropy detected (possible packing)')
        
        # Many URLs suspicious
        if len(features) > 13 and features[13] > 5:
            score += 0.2
            result['explanation'].append('Multiple URLs found')
        
        # Large file size
        if len(features) > 0 and features[0] > 1000000:
            score += 0.1
            result['explanation'].append('Large file size')
        
        result['confidence'] = min(score, 1.0)
        result['is_malicious'] = score > 0.5
        
        return result


class VisualizationDataGenerator:
    """
    Generate data for advanced visualizations.
    """
    
    @staticmethod
    def generate_entropy_map(file_path: str, block_size: int = 256) -> List[Dict]:
        """Generate entropy map data for visualization."""
        entropy_map = []
        
        try:
            with open(file_path, 'rb') as f:
                data = f.read()
            
            offset = 0
            while offset < len(data):
                block = data[offset:offset + block_size]
                if len(block) > 0:
                    counter = Counter(block)
                    entropy = 0
                    for count in counter.values():
                        p = count / len(block)
                        if p > 0:
                            entropy -= p * math.log2(p)
                    
                    entropy_map.append({
                        'offset': offset,
                        'entropy': entropy,
                        'size': len(block)
                    })
                
                offset += block_size
        
        except Exception as e:
            logger.error(f"Error generating entropy map: {e}")
        
        return entropy_map
    
    @staticmethod
    def generate_byte_distribution(file_path: str) -> Dict:
        """Generate byte distribution for visualization."""
        distribution = {i: 0 for i in range(256)}
        
        try:
            with open(file_path, 'rb') as f:
                data = f.read()
            
            for byte in data:
                distribution[byte] += 1
        
        except Exception as e:
            logger.error(f"Error generating byte distribution: {e}")
        
        return distribution


# Convenience function for comprehensive analysis
def perform_advanced_analysis(file_path: str) -> Dict:
    """
    Perform comprehensive advanced analysis on a file.
    
    Args:
        file_path: Path to file to analyze
    
    Returns:
        Complete analysis results
    """
    results = {
        'pe_analysis': None,
        'yara_scan': None,
        'ml_prediction': None,
        'entropy_map': None,
    }
    
    try:
        # PE Analysis
        pe_analyzer = PEAnalyzer(file_path)
        results['pe_analysis'] = pe_analyzer.analyze()
        
        # YARA Scan
        yara_scanner = YARAScanner()
        results['yara_scan'] = yara_scanner.scan_file(file_path)
        
        # ML Prediction
        ml_detector = MLDetector()
        features = ml_detector.extract_features(file_path)
        results['ml_prediction'] = ml_detector.predict(features)
        
        # Visualization data
        viz_gen = VisualizationDataGenerator()
        results['entropy_map'] = viz_gen.generate_entropy_map(file_path)[:20]  # Limit for JSON size
        
    except Exception as e:
        logger.error(f"Error in advanced analysis: {e}")
        results['error'] = str(e)
    
    return results
