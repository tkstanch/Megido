"""
Machine Learning Detection Engine for Malware Analyzer

Provides:
- 200+ feature extraction from binary files
- RandomForest + GradientBoosting ensemble classifier
- Model persistence (save/load via joblib)
- Graceful fallback to heuristic scoring when scikit-learn is unavailable
- Feature categories: byte histogram, entropy, n-gram statistics,
  import/export analysis, section characteristics, string statistics
"""

import logging
import math
import os
import re
import struct
import hashlib
from collections import Counter
from typing import Dict, List, Optional, Any, Tuple

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Optional dependency guards
# ---------------------------------------------------------------------------

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    logger.info("numpy not available; using pure-Python feature extraction")

try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import Pipeline
    from sklearn.calibration import CalibratedClassifierCV
    SKLEARN_AVAILABLE = True
    logger.info("scikit-learn available; using ML detection engine")
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("scikit-learn not available; falling back to heuristic detection")

try:
    import joblib
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False
    logger.info("joblib not available; model persistence disabled")

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

_SUSPICIOUS_IMPORT_KEYWORDS = [
    "virtualalloc", "virtualallocex", "writeprocessmemory",
    "createremotethread", "ntcreatethread", "rtlcreateuserthr",
    "queueuserapc", "sethooks", "setwindowshookex",
    "internetopen", "internetconnect", "httpsendrequesta",
    "urldownloadtofile", "winhttpopen",
    "createprocess", "shellexecute", "winexec",
    "regsetvalue", "regcreatekey",
    "getprocaddress", "loadlibrary", "freelibrary",
    "isdebuggerpresent", "checkremotedebugger", "ntqueryinformation",
    "getasynckeystate", "setwindowshook",
    "cryptencrypt", "cryptdecrypt", "cryptcreate",
    "socket", "connect", "send", "recv", "bind",
]

_SUSPICIOUS_STRINGS = [
    b"cmd.exe", b"powershell", b"mshta", b"wscript", b"cscript",
    b"/bin/sh", b"/bin/bash", b"wget", b"curl",
    b"base64", b"eval(", b"exec(",
    b"CreateObject", b"WScript.Shell",
    b"HKEY_LOCAL_MACHINE", b"HKEY_CURRENT_USER",
    b"SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run",
]

_PACKER_SECTIONS = {".upx0", ".upx1", ".upx2", ".vmp0", ".vmp1", ".themida", ".aspack"}

# ---------------------------------------------------------------------------
# Pure-Python helpers
# ---------------------------------------------------------------------------

def _entropy(data: bytes) -> float:
    """Shannon entropy."""
    if not data:
        return 0.0
    counts = Counter(data)
    length = len(data)
    return -sum((c / length) * math.log2(c / length) for c in counts.values() if c)


def _byte_histogram(data: bytes, normalize: bool = True) -> List[float]:
    """256-element byte frequency histogram."""
    counts = Counter(data)
    hist = [counts.get(i, 0) for i in range(256)]
    if normalize and data:
        total = len(data)
        hist = [c / total for c in hist]
    return hist


def _bigram_counts(data: bytes, top_n: int = 16) -> List[float]:
    """Top-N most frequent byte bigrams (as normalised counts)."""
    total = max(1, len(data) - 1)
    counts: Counter = Counter()
    for i in range(len(data) - 1):
        counts[(data[i], data[i + 1])] += 1
    top = counts.most_common(top_n)
    # Pad to top_n entries
    values = [c / total for _, c in top]
    values += [0.0] * (top_n - len(values))
    return values


def _printable_string_stats(data: bytes) -> Dict[str, float]:
    """Statistics over printable ASCII string runs."""
    strings = re.findall(rb"[ -~]{4,}", data)
    if not strings:
        return {"count": 0, "avg_len": 0.0, "max_len": 0.0, "url_count": 0.0, "ip_count": 0.0}
    lengths = [len(s) for s in strings]
    url_count = sum(1 for s in strings if b"http" in s.lower())
    ip_pattern = re.compile(rb"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}")
    ip_count = sum(1 for s in strings if ip_pattern.search(s))
    return {
        "count": float(len(strings)),
        "avg_len": sum(lengths) / len(lengths),
        "max_len": float(max(lengths)),
        "url_count": float(url_count),
        "ip_count": float(ip_count),
    }


# ---------------------------------------------------------------------------
# Feature Extractor
# ---------------------------------------------------------------------------

class FeatureExtractor:
    """
    Extracts 200+ numeric features from a binary file for ML classification.

    Feature groups:
    1.  File-level statistics (size, entropy, …)             [4 features]
    2.  Byte histogram (256 byte frequencies)                [256 features]
    3.  Byte bigrams – top 16                                [16 features]
    4.  Section features (PE-aware, counts + per-section)    [12 features]
    5.  Import/export features                               [10 features]
    6.  String statistics                                    [5 features]
    7.  Header anomaly flags                                 [10 features]
    Total: ~313 features (may vary with PE availability)
    """

    def __init__(self):
        try:
            import pefile
            self._pefile = pefile
            self._pe_available = True
        except ImportError:
            self._pe_available = False

    def extract(self, file_path: str) -> List[float]:
        """
        Extract features from *file_path* and return a flat list of floats.

        :param file_path: Path to binary file.
        :returns: Feature vector as a list of floats.
        """
        try:
            with open(file_path, "rb") as fh:
                data = fh.read()
        except OSError as exc:
            logger.error("Feature extraction read error: %s", exc)
            return [0.0] * 313

        features: List[float] = []

        # 1. File-level
        features += self._file_level_features(data, file_path)

        # 2. Byte histogram (256)
        features += _byte_histogram(data, normalize=True)

        # 3. Byte bigrams (16)
        features += _bigram_counts(data, top_n=16)

        # 4. Section features (PE)
        features += self._section_features(data)

        # 5. Import / export features
        features += self._import_features(data)

        # 6. String statistics (5)
        stats = _printable_string_stats(data)
        features += [
            stats["count"] / max(1, len(data) / 1000),  # normalised by kB
            stats["avg_len"],
            stats["max_len"],
            stats["url_count"],
            stats["ip_count"],
        ]

        # 7. Header anomaly flags (10)
        features += self._header_anomaly_flags(data)

        return features

    # ------------------------------------------------------------------

    def _file_level_features(self, data: bytes, path: str) -> List[float]:
        size = len(data)
        ent = _entropy(data)
        # Ratio of zero bytes
        zero_ratio = data.count(0) / max(1, size)
        # High-byte ratio (>= 0x80) – packed/encrypted
        high_ratio = sum(1 for b in data if b >= 0x80) / max(1, size)
        return [float(size), ent, zero_ratio, high_ratio]

    def _section_features(self, data: bytes) -> List[float]:
        """12 PE section-related features with graceful fallback."""
        if not self._pe_available:
            return [0.0] * 12
        try:
            pe = self._pefile.PE(data=data, fast_load=False)
            sections = pe.sections
            n_sections = float(len(sections))
            entropies = [_entropy(s.get_data()) for s in sections]
            max_ent = max(entropies) if entropies else 0.0
            avg_ent = sum(entropies) / max(1, len(entropies))
            high_ent_count = float(sum(1 for e in entropies if e > 7.0))

            exec_sections = float(sum(
                1 for s in sections
                if s.Characteristics & 0x20000000  # IMAGE_SCN_MEM_EXECUTE
            ))
            writeable_exec = float(sum(
                1 for s in sections
                if (s.Characteristics & 0x20000000)
                and (s.Characteristics & 0x80000000)
            ))
            packer_sections = float(sum(
                1 for s in sections
                if s.Name.decode("latin-1").strip("\x00").lower() in _PACKER_SECTIONS
            ))
            size_mismatch = float(sum(
                1 for s in sections
                if abs(s.SizeOfRawData - s.Misc_VirtualSize) > s.Misc_VirtualSize * 0.5
            ))
            # Virtual size sum / raw size sum
            raw_sum = sum(s.SizeOfRawData for s in sections) or 1
            virt_sum = sum(s.Misc_VirtualSize for s in sections)
            virt_raw_ratio = virt_sum / raw_sum

            return [
                n_sections, max_ent, avg_ent, high_ent_count,
                exec_sections, writeable_exec, packer_sections,
                size_mismatch, virt_raw_ratio,
                0.0, 0.0, 0.0,  # reserved for future use
            ]
        except Exception:
            return [0.0] * 12

    def _import_features(self, data: bytes) -> List[float]:
        """10 import/export–related features."""
        if not self._pe_available:
            return self._heuristic_import_features(data)
        try:
            pe = self._pefile.PE(data=data, fast_load=False)
            imports: List[str] = []
            if hasattr(pe, "DIRECTORY_ENTRY_IMPORT"):
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    for imp in entry.imports:
                        if imp.name:
                            imports.append(imp.name.decode("latin-1", errors="replace").lower())

            exports: List[str] = []
            if hasattr(pe, "DIRECTORY_ENTRY_EXPORT"):
                for exp in pe.DIRECTORY_ENTRY_EXPORT.symbols:
                    if exp.name:
                        exports.append(exp.name.decode("latin-1", errors="replace").lower())

            n_imports = float(len(imports))
            n_exports = float(len(exports))
            suspicious_imports = float(sum(
                1 for imp in imports
                if any(kw in imp for kw in _SUSPICIOUS_IMPORT_KEYWORDS)
            ))
            has_internet = float(any(
                kw in imp for imp in imports
                for kw in ("internet", "winhttp", "urldownload", "socket")
            ))
            has_crypto = float(any(
                kw in imp for imp in imports
                for kw in ("crypt", "bcrypt", "ncrypt")
            ))
            has_process_inject = float(any(
                kw in imp for imp in imports
                for kw in ("virtualallocex", "writeprocessmemory", "createremotethread")
            ))
            has_registry = float(any(
                kw in imp for imp in imports
                for kw in ("regsetvalue", "regcreatekey", "regopen")
            ))
            has_anti_debug = float(any(
                kw in imp for imp in imports
                for kw in ("isdebuggerpresent", "checkremotedebugger")
            ))
            # n-gram overlap with suspicious string list in imports
            suspicious_string_hits = float(sum(
                1 for s in _SUSPICIOUS_STRINGS if s in data
            ))
            return [
                n_imports, n_exports, suspicious_imports,
                has_internet, has_crypto, has_process_inject,
                has_registry, has_anti_debug, suspicious_string_hits,
                0.0,  # reserved
            ]
        except Exception:
            return self._heuristic_import_features(data)

    def _heuristic_import_features(self, data: bytes) -> List[float]:
        data_lower = data.lower()
        suspicious_hits = float(sum(1 for s in _SUSPICIOUS_STRINGS if s.lower() in data_lower))
        return [0.0, 0.0, suspicious_hits, 0.0, 0.0, 0.0, 0.0, 0.0, suspicious_hits, 0.0]

    def _header_anomaly_flags(self, data: bytes) -> List[float]:
        """10 binary flags for known header anomalies."""
        flags = []

        # Flag 0: MZ header present
        flags.append(1.0 if data[:2] == b"MZ" else 0.0)

        # Flag 1: PE checksum mismatch
        if self._pe_available and data[:2] == b"MZ":
            try:
                pe = self._pefile.PE(data=data, fast_load=True)
                stored = pe.OPTIONAL_HEADER.CheckSum
                calculated = pe.generate_checksum()
                flags.append(1.0 if stored != 0 and stored != calculated else 0.0)
            except Exception:
                flags.append(0.0)
        else:
            flags.append(0.0)

        # Flag 2: Compile timestamp is zero
        if self._pe_available and data[:2] == b"MZ":
            try:
                pe = self._pefile.PE(data=data, fast_load=True)
                flags.append(1.0 if pe.FILE_HEADER.TimeDateStamp == 0 else 0.0)
            except Exception:
                flags.append(0.0)
        else:
            flags.append(0.0)

        # Flag 3: Very new compile timestamp (future date) – reserved for PE timestamp parsing
        flags.append(0.0)

        # Flag 4: ELF magic
        flags.append(1.0 if data[:4] == b"\x7fELF" else 0.0)

        # Flag 5: ZIP/APK magic
        flags.append(1.0 if data[:2] == b"PK" else 0.0)

        # Flag 6: OLE2 magic
        flags.append(1.0 if data[:8] == b"\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1" else 0.0)

        # Flag 7: PDF magic
        flags.append(1.0 if data[:4] == b"%PDF" else 0.0)

        # Flag 8: Overlay data present (PE file has data after last section)
        if self._pe_available and data[:2] == b"MZ":
            try:
                pe = self._pefile.PE(data=data, fast_load=True)
                overlay_off = pe.get_overlay_data_start_offset()
                flags.append(1.0 if overlay_off and overlay_off < len(data) else 0.0)
            except Exception:
                flags.append(0.0)
        else:
            flags.append(0.0)

        # Flag 9: File entropy > 7.0
        flags.append(1.0 if _entropy(data) > 7.0 else 0.0)

        return flags

    def feature_names(self) -> List[str]:
        """Return human-readable feature names (for model introspection)."""
        names = ["file_size", "file_entropy", "zero_byte_ratio", "high_byte_ratio"]
        names += [f"byte_{i:03d}" for i in range(256)]
        names += [f"bigram_{i}" for i in range(16)]
        names += [
            "n_sections", "max_section_entropy", "avg_section_entropy",
            "high_entropy_sections", "exec_sections", "writable_exec_sections",
            "packer_sections", "section_size_mismatch", "virt_raw_ratio",
            "sec_reserved_0", "sec_reserved_1", "sec_reserved_2",
        ]
        names += [
            "n_imports", "n_exports", "suspicious_imports",
            "has_internet_api", "has_crypto_api", "has_process_inject_api",
            "has_registry_api", "has_anti_debug_api", "suspicious_string_hits",
            "imp_reserved_0",
        ]
        names += [
            "str_count_norm", "str_avg_len", "str_max_len",
            "str_url_count", "str_ip_count",
        ]
        names += [
            "has_mz_header", "pe_checksum_mismatch", "zero_timestamp",
            "future_timestamp", "is_elf", "is_zip", "is_ole", "is_pdf",
            "has_overlay", "high_entropy_file",
        ]
        return names


# ---------------------------------------------------------------------------
# Heuristic Scorer (fallback)
# ---------------------------------------------------------------------------

class HeuristicScorer:
    """
    Rule-based malware probability scorer used when scikit-learn is unavailable.

    Produces a 0–1 probability score and a set of triggered rule names.
    """

    RULES: List[Tuple[str, float, Any]] = [
        # (rule_name, score_contribution, check_fn)
        ("high_file_entropy",    0.15, lambda f: f["file_entropy"] > 7.0),
        ("packer_sections",      0.20, lambda f: f["packer_sections"] > 0),
        ("high_entropy_section", 0.10, lambda f: f["high_entropy_sections"] > 0),
        ("process_injection",    0.25, lambda f: f["has_process_inject_api"] > 0),
        ("anti_debug",           0.15, lambda f: f["has_anti_debug_api"] > 0),
        ("internet_api",         0.10, lambda f: f["has_internet_api"] > 0),
        ("suspicious_strings",   0.10, lambda f: f["suspicious_string_hits"] > 2),
        ("writable_exec_section",0.20, lambda f: f["writable_exec_sections"] > 0),
        ("overlay_data",         0.05, lambda f: f["has_overlay"] > 0),
        ("many_imports",         0.03, lambda f: f["n_imports"] > 200),
        ("few_imports",          0.08, lambda f: 0 < f["n_imports"] < 5),
        ("url_strings",          0.05, lambda f: f["str_url_count"] > 0),
        ("ip_strings",           0.05, lambda f: f["str_ip_count"] > 0),
    ]

    def score(self, feature_dict: Dict[str, float]) -> Tuple[float, List[str]]:
        """
        Return (probability, triggered_rules).

        :param feature_dict: Dict mapping feature names to float values.
        """
        score = 0.0
        triggered = []
        for rule_name, contrib, check in self.RULES:
            try:
                if check(feature_dict):
                    score += contrib
                    triggered.append(rule_name)
            except (KeyError, TypeError):
                pass
        return min(score, 1.0), triggered


# ---------------------------------------------------------------------------
# Ensemble ML Model Builder
# ---------------------------------------------------------------------------

class MalwareMLModel:
    """
    RandomForest + GradientBoosting soft-voting ensemble.

    Training is intentionally left to the caller (``train()`` method)
    since the model ships without pre-trained weights.  For production,
    train the model offline and persist it with ``save()``.
    """

    DEFAULT_MODEL_PATH = os.path.join(os.path.dirname(__file__), "ml_model.joblib")

    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path or self.DEFAULT_MODEL_PATH
        self._pipeline: Optional[Any] = None
        self._is_trained = False

        if SKLEARN_AVAILABLE:
            self._build_pipeline()
        else:
            logger.warning("scikit-learn not available; MalwareMLModel uses heuristic fallback")

    def _build_pipeline(self) -> None:
        rf = RandomForestClassifier(
            n_estimators=200,
            max_depth=None,
            min_samples_split=2,
            class_weight="balanced",
            random_state=42,
            n_jobs=-1,
        )
        gb = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.05,
            max_depth=5,
            subsample=0.8,
            random_state=42,
        )
        ensemble = VotingClassifier(
            estimators=[("rf", rf), ("gb", gb)],
            voting="soft",
        )
        self._pipeline = Pipeline([
            ("scaler", StandardScaler()),
            ("clf", ensemble),
        ])

    def train(self, X: List[List[float]], y: List[int]) -> Dict[str, Any]:
        """
        Train the ensemble on feature matrix X and binary labels y.

        :param X: List of feature vectors (each from FeatureExtractor.extract()).
        :param y: Binary labels (0 = benign, 1 = malicious).
        :returns: Training summary dict.
        """
        if not SKLEARN_AVAILABLE:
            return {"error": "scikit-learn not available"}
        if not X or not y:
            return {"error": "Empty training data"}

        import numpy as _np
        X_arr = _np.array(X, dtype=float)
        y_arr = _np.array(y, dtype=int)

        from sklearn.model_selection import cross_val_score
        logger.info("Training ensemble on %d samples…", len(y_arr))
        self._pipeline.fit(X_arr, y_arr)
        self._is_trained = True

        scores = cross_val_score(self._pipeline, X_arr, y_arr, cv=5, scoring="f1")
        result = {
            "samples": len(y_arr),
            "cv_f1_mean": float(scores.mean()),
            "cv_f1_std": float(scores.std()),
        }
        logger.info("Training complete: F1 mean=%.3f ±%.3f", result["cv_f1_mean"], result["cv_f1_std"])
        return result

    def predict(self, features: List[float]) -> Dict[str, Any]:
        """
        Predict maliciousness probability for a single sample.

        :param features: Feature vector from FeatureExtractor.extract().
        :returns: Dict with ``is_malicious``, ``probability``, ``confidence``.
        """
        if not SKLEARN_AVAILABLE or not self._is_trained:
            return {"error": "Model not trained or scikit-learn unavailable"}
        import numpy as _np
        X = _np.array([features], dtype=float)
        try:
            prob = self._pipeline.predict_proba(X)[0][1]
            label = int(prob >= 0.5)
            return {
                "is_malicious": bool(label),
                "probability": float(prob),
                "confidence": float(abs(prob - 0.5) * 2),  # 0=uncertain, 1=certain
            }
        except Exception as exc:
            logger.error("ML prediction error: %s", exc)
            return {"error": str(exc)}

    def save(self, path: Optional[str] = None) -> bool:
        """Persist the trained pipeline to disk."""
        if not JOBLIB_AVAILABLE:
            logger.warning("joblib not available; cannot save model")
            return False
        if not self._is_trained:
            logger.warning("Model not trained; nothing to save")
            return False
        save_path = path or self.model_path
        try:
            joblib.dump(self._pipeline, save_path)
            logger.info("ML model saved to %s", save_path)
            return True
        except Exception as exc:
            logger.error("Model save error: %s", exc)
            return False

    def load(self, path: Optional[str] = None) -> bool:
        """Load a previously saved pipeline from disk."""
        if not JOBLIB_AVAILABLE:
            logger.warning("joblib not available; cannot load model")
            return False
        load_path = path or self.model_path
        if not os.path.isfile(load_path):
            logger.info("No saved ML model found at %s", load_path)
            return False
        try:
            self._pipeline = joblib.load(load_path)
            self._is_trained = True
            logger.info("ML model loaded from %s", load_path)
            return True
        except Exception as exc:
            logger.error("Model load error: %s", exc)
            return False


# ---------------------------------------------------------------------------
# Main detection engine
# ---------------------------------------------------------------------------

class MLDetectionEngine:
    """
    Top-level ML detection engine.

    Combines feature extraction, ML ensemble, and heuristic fallback
    into a single, easy-to-use interface.

    Usage::

        engine = MLDetectionEngine()
        result = engine.analyze("/path/to/sample")
    """

    def __init__(self, model_path: Optional[str] = None):
        self.extractor = FeatureExtractor()
        self.heuristic = HeuristicScorer()
        self.model = MalwareMLModel(model_path=model_path)
        # Attempt to load a pre-trained model
        self.model.load()

    def analyze(self, file_path: str) -> Dict[str, Any]:
        """
        Analyse a file and return a comprehensive detection result.

        :param file_path: Path to the binary file.
        :returns: Dict with detection verdict, probability, features, indicators.
        """
        if not os.path.isfile(file_path):
            return {"error": f"File not found: {file_path}"}

        # Extract features
        features = self.extractor.extract(file_path)
        names = self.extractor.feature_names()
        feature_dict = dict(zip(names, features))

        # Heuristic scoring (always available)
        heuristic_prob, triggered_rules = self.heuristic.score(feature_dict)

        result: Dict[str, Any] = {
            "file_path": file_path,
            "feature_count": len(features),
            "heuristic": {
                "probability": round(heuristic_prob, 4),
                "is_suspicious": heuristic_prob >= 0.3,
                "triggered_rules": triggered_rules,
            },
            "key_features": {
                "file_entropy": round(feature_dict.get("file_entropy", 0.0), 3),
                "packer_sections": int(feature_dict.get("packer_sections", 0)),
                "suspicious_imports": int(feature_dict.get("suspicious_imports", 0)),
                "process_injection_apis": int(feature_dict.get("has_process_inject_api", 0)),
                "anti_debug_apis": int(feature_dict.get("has_anti_debug_api", 0)),
                "url_strings": int(feature_dict.get("str_url_count", 0)),
            },
        }

        # ML scoring (when model is trained)
        if SKLEARN_AVAILABLE and self.model._is_trained:
            ml_result = self.model.predict(features)
            result["ml"] = ml_result
            # Weighted combination: 60% ML, 40% heuristic
            if "probability" in ml_result:
                combined = 0.6 * ml_result["probability"] + 0.4 * heuristic_prob
                result["combined_probability"] = round(combined, 4)
                result["verdict"] = "malicious" if combined >= 0.5 else "benign"
            else:
                result["combined_probability"] = round(heuristic_prob, 4)
                result["verdict"] = "suspicious" if heuristic_prob >= 0.3 else "benign"
        else:
            result["ml"] = {"status": "not_available" if not SKLEARN_AVAILABLE else "not_trained"}
            result["combined_probability"] = round(heuristic_prob, 4)
            result["verdict"] = "suspicious" if heuristic_prob >= 0.3 else "benign"

        return result

    @property
    def engine_info(self) -> Dict[str, Any]:
        """Return metadata about the engine."""
        return {
            "sklearn_available": SKLEARN_AVAILABLE,
            "numpy_available": NUMPY_AVAILABLE,
            "joblib_available": JOBLIB_AVAILABLE,
            "model_trained": self.model._is_trained,
            "feature_count": len(self.extractor.feature_names()),
            "heuristic_rules": len(self.heuristic.RULES),
        }
