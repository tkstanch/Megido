"""
Network traffic analysis module.

Analyses PCAP/PCAPNG captures for C2 beaconing, DGA domains, TLS
fingerprints, and generates Suricata rule stubs.

Optional dependencies (graceful fallback if not installed):
  - scapy
  - dpkt
"""
import collections
import hashlib
import logging
import math
import re
from typing import Any, Dict, Iterator, List, Optional, Tuple

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Optional dependency imports with graceful fallback
# ---------------------------------------------------------------------------

try:
    import scapy.all as _scapy  # type: ignore
    from scapy.layers.dns import DNS, DNSQR  # type: ignore
    from scapy.layers.http import HTTP  # type: ignore
    from scapy.layers.inet import IP, TCP, UDP  # type: ignore
    from scapy.layers.tls.record import TLS  # type: ignore
    _HAS_SCAPY = True
except ImportError:
    _HAS_SCAPY = False
    logger.info("scapy not available; falling back to dpkt or basic parsing")

try:
    import dpkt  # type: ignore
    _HAS_DPKT = True
except ImportError:
    _HAS_DPKT = False
    if not _HAS_SCAPY:
        logger.warning("Neither scapy nor dpkt is available; PCAP analysis is limited")


# ---------------------------------------------------------------------------
# Entropy helpers
# ---------------------------------------------------------------------------

def _shannon_entropy(data: str) -> float:
    """Return the Shannon entropy (bits per character) of *data*."""
    if not data:
        return 0.0
    counts = collections.Counter(data)
    length = len(data)
    return -sum((c / length) * math.log2(c / length) for c in counts.values())


def _ngram_frequency(domain: str, n: int = 3) -> float:
    """Return the average frequency of n-grams in common English text (proxy)."""
    # Heuristic: high-entropy short n-gram set indicates DGA.
    ngrams = [domain[i:i + n] for i in range(len(domain) - n + 1)]
    if not ngrams:
        return 0.0
    unique_ratio = len(set(ngrams)) / len(ngrams)
    return unique_ratio


# ---------------------------------------------------------------------------
# PcapAnalyzer
# ---------------------------------------------------------------------------

class PcapAnalyzer:
    """
    Parse PCAP/PCAPNG files and extract network artefacts.

    Falls back to raw byte scanning when neither scapy nor dpkt is available.
    """

    def analyse(self, pcap_path: str) -> Dict:
        """
        Parse *pcap_path* and return extracted network data.

        Returns:
            Dict with keys ``ips``, ``domains``, ``urls``,
            ``dns_queries``, ``http_requests``, ``connections``.
        """
        if _HAS_SCAPY:
            return self._analyse_scapy(pcap_path)
        if _HAS_DPKT:
            return self._analyse_dpkt(pcap_path)
        return self._analyse_raw(pcap_path)

    # ------------------------------------------------------------------
    def _analyse_scapy(self, pcap_path: str) -> Dict:
        ips: List[str] = []
        domains: List[str] = []
        urls: List[str] = []
        dns_queries: List[str] = []
        http_requests: List[Dict] = []
        connections: List[str] = []

        try:
            packets = _scapy.rdpcap(pcap_path)
        except Exception as exc:
            logger.error("scapy failed to read %s: %s", pcap_path, exc)
            return self._analyse_raw(pcap_path)

        for pkt in packets:
            try:
                if pkt.haslayer(IP):
                    src = pkt[IP].src
                    dst = pkt[IP].dst
                    if src not in ips:
                        ips.append(src)
                    if dst not in ips:
                        ips.append(dst)
                    if pkt.haslayer(TCP) or pkt.haslayer(UDP):
                        sport = pkt.sport
                        dport = pkt.dport
                        conn = f"{src}:{sport} -> {dst}:{dport}"
                        if conn not in connections:
                            connections.append(conn)

                if pkt.haslayer(DNS) and pkt.haslayer(DNSQR):
                    qname = pkt[DNSQR].qname
                    if isinstance(qname, bytes):
                        qname = qname.decode("utf-8", errors="replace").rstrip(".")
                    if qname and qname not in dns_queries:
                        dns_queries.append(qname)
                        if qname not in domains:
                            domains.append(qname)

                if pkt.haslayer(HTTP):
                    layer = pkt[HTTP]
                    host = getattr(layer, "Host", b"")
                    path = getattr(layer, "Path", b"")
                    if isinstance(host, bytes):
                        host = host.decode("utf-8", errors="replace")
                    if isinstance(path, bytes):
                        path = path.decode("utf-8", errors="replace")
                    if host:
                        url = f"http://{host}{path}"
                        if url not in urls:
                            urls.append(url)
                        req = {"method": getattr(layer, "Method", b"").decode("utf-8", errors="replace"),
                               "host": host, "path": path}
                        http_requests.append(req)
            except Exception:
                pass  # skip malformed packets

        return {
            "ips": ips,
            "domains": domains,
            "urls": urls,
            "dns_queries": dns_queries,
            "http_requests": http_requests,
            "connections": connections,
        }

    # ------------------------------------------------------------------
    def _analyse_dpkt(self, pcap_path: str) -> Dict:
        ips: List[str] = []
        domains: List[str] = []
        urls: List[str] = []
        dns_queries: List[str] = []
        connections: List[str] = []
        import socket as _socket

        try:
            with open(pcap_path, "rb") as f:
                try:
                    cap = dpkt.pcap.Reader(f)
                except Exception:
                    f.seek(0)
                    cap = dpkt.pcapng.Reader(f)
                for _ts, buf in cap:
                    try:
                        eth = dpkt.ethernet.Ethernet(buf)
                        if not isinstance(eth.data, dpkt.ip.IP):
                            continue
                        ip = eth.data
                        src = _socket.inet_ntoa(ip.src)
                        dst = _socket.inet_ntoa(ip.dst)
                        if src not in ips:
                            ips.append(src)
                        if dst not in ips:
                            ips.append(dst)
                        if isinstance(ip.data, (dpkt.tcp.TCP, dpkt.udp.UDP)):
                            transport = ip.data
                            conn = f"{src}:{transport.sport} -> {dst}:{transport.dport}"
                            if conn not in connections:
                                connections.append(conn)
                        if isinstance(ip.data, dpkt.udp.UDP):
                            try:
                                dns_pkt = dpkt.dns.DNS(ip.data.data)
                                for q in dns_pkt.qd:
                                    name = q.name
                                    if name and name not in dns_queries:
                                        dns_queries.append(name)
                                        domains.append(name)
                            except Exception:
                                pass
                    except Exception:
                        pass
        except Exception as exc:
            logger.error("dpkt failed to read %s: %s", pcap_path, exc)
            return self._analyse_raw(pcap_path)

        return {
            "ips": ips,
            "domains": domains,
            "urls": urls,
            "dns_queries": dns_queries,
            "http_requests": [],
            "connections": connections,
        }

    # ------------------------------------------------------------------
    @staticmethod
    def _analyse_raw(pcap_path: str) -> Dict:
        """Fallback: extract IOCs from raw file bytes using regex."""
        ip_re = re.compile(
            rb"(?:(?:25[0-5]|2[0-4]\d|[01]?\d\d?)\.){3}(?:25[0-5]|2[0-4]\d|[01]?\d\d?)"
        )
        url_re = re.compile(rb"https?://[^\x00\r\n \"'<>]{4,256}")
        domain_re = re.compile(
            rb"(?:[a-zA-Z0-9](?:[a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+"
            rb"(?:com|net|org|info|biz|ru|cn|io)"
            rb"(?=[\x00\r\n\x09 \"'<>]|$)"
        )
        try:
            with open(pcap_path, "rb") as f:
                data = f.read()
        except OSError as exc:
            return {"error": str(exc)}

        ips = list({m.group(0).decode("ascii") for m in ip_re.finditer(data)})
        urls = list({m.group(0).decode("ascii", errors="replace") for m in url_re.finditer(data)})
        domains = list({m.group(0).decode("ascii", errors="replace") for m in domain_re.finditer(data)})
        return {
            "ips": ips,
            "domains": domains,
            "urls": urls,
            "dns_queries": [],
            "http_requests": [],
            "connections": [],
        }


# ---------------------------------------------------------------------------
# C2Detector
# ---------------------------------------------------------------------------

class C2Detector:
    """Detect command-and-control communication patterns in network data."""

    # Beaconing: connections with suspiciously regular inter-arrival times.
    _BEACON_JITTER_THRESHOLD = 0.15  # < 15 % coefficient of variation â†’ regular

    def detect_beaconing(self, connection_timestamps: Dict[str, List[float]]) -> List[Dict]:
        """
        Identify connections that exhibit beaconing behaviour.

        Args:
            connection_timestamps: Mapping of ``"src_ip:dst_ip:dst_port"``
                to a list of float Unix timestamps.

        Returns:
            List of dicts with ``connection``, ``interval_mean``,
            ``interval_cv``, and ``beacon_count``.
        """
        beacons: List[Dict] = []
        for conn, timestamps in connection_timestamps.items():
            if len(timestamps) < 4:
                continue
            ts_sorted = sorted(timestamps)
            intervals = [ts_sorted[i + 1] - ts_sorted[i] for i in range(len(ts_sorted) - 1)]
            if not intervals:
                continue
            mean = sum(intervals) / len(intervals)
            if mean <= 0:
                continue
            variance = sum((x - mean) ** 2 for x in intervals) / len(intervals)
            cv = math.sqrt(variance) / mean  # coefficient of variation
            if cv < self._BEACON_JITTER_THRESHOLD:
                beacons.append({
                    "connection": conn,
                    "interval_mean_s": round(mean, 2),
                    "interval_cv": round(cv, 4),
                    "beacon_count": len(timestamps),
                })
        return beacons

    def detect_http_c2(self, http_requests: List[Dict]) -> List[Dict]:
        """
        Identify HTTP requests matching common C2 URI patterns.

        Args:
            http_requests: List of dicts with at least ``host`` and ``path``.

        Returns:
            Suspicious requests annotated with ``c2_indicator`` field.
        """
        # Common C2 URI patterns (Cobalt Strike, Metasploit, etc.)
        _C2_PATH_RE = re.compile(
            r"(?:^/[a-zA-Z0-9]{4,8}$"          # short random path
            r"|/ca$|/updates$|/news$"
            r"|/image\.\w+$"                   # fake image
            r"|/jquery-\d[\d.]*\.min\.js$"     # fake JS
            r"|/submit\.php$|/gate\.php$|/panel\.php$"
            r")"
        )
        suspicious: List[Dict] = []
        for req in http_requests:
            path = req.get("path", "")
            if _C2_PATH_RE.search(path):
                req_copy = dict(req)
                req_copy["c2_indicator"] = "suspicious_path"
                suspicious.append(req_copy)
        return suspicious

    def detect_dns_tunneling(self, dns_queries: List[str]) -> List[Dict]:
        """
        Detect potential DNS tunnelling by analysing query entropy.

        Args:
            dns_queries: List of FQDN query strings.

        Returns:
            List of suspicious queries annotated with entropy and reason.
        """
        suspicious: List[Dict] = []
        for query in dns_queries:
            labels = query.split(".")
            if not labels:
                continue
            # The subdomain portion is everything except the last two labels.
            subdomain = ".".join(labels[:-2]) if len(labels) > 2 else labels[0]
            if len(subdomain) < 10:
                continue
            entropy = _shannon_entropy(subdomain)
            if entropy > 3.5:  # high entropy indicates encoded data
                suspicious.append({
                    "query": query,
                    "subdomain_entropy": round(entropy, 3),
                    "reason": "high_entropy_subdomain",
                })
        return suspicious


# ---------------------------------------------------------------------------
# DGADetector
# ---------------------------------------------------------------------------

class DGADetector:
    """Detect algorithmically generated domains (DGA) using entropy and n-grams."""

    # Domains with entropy above this are considered DGA candidates.
    _ENTROPY_THRESHOLD = 3.5
    # Very high unique n-gram ratio suggests randomness.
    _NGRAM_THRESHOLD = 0.85
    # Minimum domain label length to analyse.
    _MIN_LABEL_LEN = 8

    def analyse_domains(self, domains: List[str]) -> List[Dict]:
        """
        Score each domain and flag potential DGA domains.

        Returns:
            List of dicts with ``domain``, ``entropy``, ``ngram_ratio``,
            and ``is_dga`` for flagged domains only.
        """
        flagged: List[Dict] = []
        for domain in domains:
            label = domain.split(".")[0]  # analyse the leftmost label
            if len(label) < self._MIN_LABEL_LEN:
                continue
            entropy = _shannon_entropy(label)
            ngram_ratio = _ngram_frequency(label)
            if entropy > self._ENTROPY_THRESHOLD or ngram_ratio > self._NGRAM_THRESHOLD:
                flagged.append({
                    "domain": domain,
                    "entropy": round(entropy, 3),
                    "ngram_ratio": round(ngram_ratio, 3),
                    "is_dga": True,
                })
        return flagged


# ---------------------------------------------------------------------------
# TLSAnalyzer
# ---------------------------------------------------------------------------

def _compute_ja3(tls_record_bytes: bytes) -> str:
    """
    Compute a JA3 fingerprint from raw TLS Client Hello bytes.

    This is a lightweight implementation covering the most common fields.
    """
    # Parse minimal ClientHello: version, cipher suites, extensions.
    try:
        pos = 0
        if tls_record_bytes[pos] != 0x16:  # Content type = handshake
            return ""
        pos += 5  # skip record header
        if tls_record_bytes[pos] != 0x01:  # Handshake type = ClientHello
            return ""
        pos += 4  # type + length
        client_version = int.from_bytes(tls_record_bytes[pos: pos + 2], "big")
        pos += 34  # version + random

        session_id_len = tls_record_bytes[pos]
        pos += 1 + session_id_len

        cs_len = int.from_bytes(tls_record_bytes[pos: pos + 2], "big")
        pos += 2
        ciphers = []
        for i in range(0, cs_len, 2):
            cs = int.from_bytes(tls_record_bytes[pos + i: pos + i + 2], "big")
            if cs not in (0x0000, 0x00FF):  # skip SCSV values
                ciphers.append(cs)
        pos += cs_len

        comp_len = tls_record_bytes[pos]
        pos += 1 + comp_len

        # Extensions.
        ext_types: List[int] = []
        elliptic_curves: List[int] = []
        ec_point_formats: List[int] = []

        if pos + 2 <= len(tls_record_bytes):
            ext_total = int.from_bytes(tls_record_bytes[pos: pos + 2], "big")
            pos += 2
            ext_end = pos + ext_total
            while pos + 4 <= ext_end:
                ext_type = int.from_bytes(tls_record_bytes[pos: pos + 2], "big")
                ext_len = int.from_bytes(tls_record_bytes[pos + 2: pos + 4], "big")
                ext_types.append(ext_type)
                if ext_type == 0x000A and pos + 4 + ext_len <= len(tls_record_bytes):
                    # Supported groups / elliptic curves
                    curves_len = int.from_bytes(tls_record_bytes[pos + 4: pos + 6], "big")
                    for i in range(0, curves_len, 2):
                        c = int.from_bytes(tls_record_bytes[pos + 6 + i: pos + 8 + i], "big")
                        elliptic_curves.append(c)
                if ext_type == 0x000B and pos + 4 + ext_len <= len(tls_record_bytes):
                    fmt_len = tls_record_bytes[pos + 4]
                    ec_point_formats = list(tls_record_bytes[pos + 5: pos + 5 + fmt_len])
                pos += 4 + ext_len

        ja3_str = (
            f"{client_version},"
            f"{'-'.join(str(c) for c in ciphers)},"
            f"{'-'.join(str(e) for e in ext_types)},"
            f"{'-'.join(str(c) for c in elliptic_curves)},"
            f"{'-'.join(str(f) for f in ec_point_formats)}"
        )
        return hashlib.md5(ja3_str.encode()).hexdigest()
    except Exception:
        return ""


class TLSAnalyzer:
    """Extract JA3 fingerprints and certificate details from network data."""

    def extract_ja3(self, raw_packets: List[bytes]) -> List[str]:
        """
        Compute JA3 fingerprints from a list of raw TLS record bytes.

        Args:
            raw_packets: List of raw packet bytes (expected to start at the
                TLS record layer).

        Returns:
            List of unique non-empty JA3 fingerprint strings.
        """
        results: List[str] = []
        for pkt in raw_packets:
            fp = _compute_ja3(pkt)
            if fp and fp not in results:
                results.append(fp)
        return results

    def extract_certificate_info(self, raw_cert_bytes: bytes) -> Dict:
        """
        Extract basic information from a DER-encoded X.509 certificate.

        Returns an empty dict when parsing fails.
        """
        try:
            from cryptography import x509  # type: ignore
            from cryptography.hazmat.backends import default_backend  # type: ignore
            cert = x509.load_der_x509_certificate(raw_cert_bytes, default_backend())
            return {
                "subject": cert.subject.rfc4514_string(),
                "issuer": cert.issuer.rfc4514_string(),
                "not_before": cert.not_valid_before_utc.isoformat(),
                "not_after": cert.not_valid_after_utc.isoformat(),
                "serial": str(cert.serial_number),
            }
        except ImportError:
            logger.debug("cryptography library not available for certificate parsing")
        except Exception as exc:
            logger.debug("Certificate parsing failed: %s", exc)
        return {}


# ---------------------------------------------------------------------------
# Suricata Rule Generator
# ---------------------------------------------------------------------------

def _generate_suricata_rules(c2_patterns: List[Dict]) -> List[str]:
    """
    Generate Suricata IDS rule stubs from detected C2 connection patterns.

    Args:
        c2_patterns: List of dicts from :class:`C2Detector` (beaconing or
            HTTP C2 detections).

    Returns:
        List of Suricata rule strings.
    """
    rules: List[str] = []
    for i, pattern in enumerate(c2_patterns[:20], start=1):
        sid = 9_000_000 + i

        if "connection" in pattern:
            # Beaconing pattern: "src:port -> dst:port"
            parts = pattern["connection"].split(" -> ")
            if len(parts) == 2:
                dst_part = parts[1]
                dst_ip, dst_port = dst_part.rsplit(":", 1) if ":" in dst_part else (dst_part, "any")
                rule = (
                    f'alert tcp any any -> {dst_ip} {dst_port} '
                    f'(msg:"Megido C2 Beaconing to {dst_ip}:{dst_port}"; '
                    f'flow:established,to_server; '
                    f'threshold: type both, track by_src, count 5, seconds 300; '
                    f'sid:{sid}; rev:1;)'
                )
                rules.append(rule)

        elif "host" in pattern and "path" in pattern:
            host = pattern["host"].replace('"', '\\"')
            path = pattern["path"].replace('"', '\\"')
            rule = (
                f'alert http any any -> any 80 '
                f'(msg:"Megido Suspicious HTTP C2 {host}{path}"; '
                f'flow:established,to_server; '
                f'http.host; content:"{host}"; '
                f'http.uri; content:"{path}"; '
                f'sid:{sid}; rev:1;)'
            )
            rules.append(rule)

    return rules


# ---------------------------------------------------------------------------
# NetworkAnalysisEngine
# ---------------------------------------------------------------------------

class NetworkAnalysisEngine:
    """
    Top-level network analysis engine.

    Orchestrates :class:`PcapAnalyzer`, :class:`C2Detector`,
    :class:`DGADetector`, and :class:`TLSAnalyzer`.
    """

    def __init__(self) -> None:
        self.pcap_analyzer = PcapAnalyzer()
        self.c2_detector = C2Detector()
        self.dga_detector = DGADetector()
        self.tls_analyzer = TLSAnalyzer()

    def analyse_pcap(self, pcap_path: str) -> Dict:
        """
        Perform a complete network analysis of *pcap_path*.

        Returns:
            Dict with keys ``traffic``, ``beaconing``, ``http_c2``,
            ``dns_tunneling``, ``dga_domains``, ``suricata_rules``.
        """
        traffic = self.pcap_analyzer.analyse(pcap_path)

        beaconing: List[Dict] = []
        http_c2 = self.c2_detector.detect_http_c2(traffic.get("http_requests", []))
        dns_tunneling = self.c2_detector.detect_dns_tunneling(traffic.get("dns_queries", []))
        dga_domains = self.dga_detector.analyse_domains(traffic.get("domains", []))

        # Generate Suricata rules from detected C2 patterns.
        suricata_rules = _generate_suricata_rules(http_c2 + beaconing)

        return {
            "traffic": traffic,
            "beaconing": beaconing,
            "http_c2": http_c2,
            "dns_tunneling": dns_tunneling,
            "dga_domains": dga_domains,
            "suricata_rules": suricata_rules,
            "summary": {
                "total_ips": len(traffic.get("ips", [])),
                "total_domains": len(traffic.get("domains", [])),
                "total_urls": len(traffic.get("urls", [])),
                "dga_count": len(dga_domains),
                "c2_indicators": len(http_c2) + len(beaconing),
                "dns_tunnel_indicators": len(dns_tunneling),
            },
        }
