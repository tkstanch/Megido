# Enhanced Vulnerability Scanner Documentation

## Overview

The enhanced vulnerability scanner (`sensitive_scanner_enhanced.py`) provides advanced detection capabilities for sensitive information with pluggable architecture, hybrid scanning, heuristic detection, ML integration templates, and performance optimizations.

## Key Features

### 1. Pluggable Pattern Architecture
- **Base `PatternProvider` class**: Abstract base for creating custom pattern sources
- **Built-in `SensitivePatterns`**: Comprehensive set of patterns with severity classification
- **`ExternalPatternProvider`**: Load patterns from JSON files or URLs with caching

### 2. Hybrid Detection
- **Static File Scanning**: Scan local files and directories
- **Dynamic URL Scanning**: Scan web content from URLs
- **Unified Interface**: Same API for both scan types

### 3. Heuristic Detection
- **Entropy Analysis**: Detect high-entropy strings that may be tokens/secrets
- **Suspicious Assignments**: Identify variable assignments with suspicious keywords
- **Pattern-Independent**: Catch unknown sensitive patterns

### 4. ML Integration Template
- **Stub Implementation**: Ready-to-use template for ML models
- **Pluggable Design**: Easy integration with sklearn, TensorFlow, etc.
- **Feature Extraction**: Template methods for feature engineering

### 5. Performance Optimizations
- **Configurable Thread Pools**: Adjust concurrency based on workload
- **Result Caching**: Avoid redundant scans with TTL-based cache
- **Parallel Scanning**: Concurrent URL and file processing

### 6. Context Awareness
- **Environment Correlation**: Check findings against environment variables
- **Config File Detection**: Identify sensitive data in configuration files
- **Risk Assessment**: Enhanced severity for config file findings

### 7. Enhanced Logging
- **Configurable Levels**: INFO, DEBUG, WARNING support
- **Severity Classification**: Critical, High, Medium, Low
- **Structured Output**: Detailed findings with metadata

## Usage Examples

### Basic URL Scanning (Backward Compatible)

```python
from discover.sensitive_scanner_enhanced import EnhancedSensitiveInfoScanner

# Initialize scanner with defaults
scanner = EnhancedSensitiveInfoScanner()

# Scan a single URL
result = scanner.scan_url('https://example.com')
print(f"Found {len(result['findings'])} issues")

# Scan multiple URLs
urls = ['https://site1.com', 'https://site2.com']
results = scanner.scan_urls(urls)
```

### File and Directory Scanning

```python
from discover.sensitive_scanner_enhanced import EnhancedSensitiveInfoScanner

scanner = EnhancedSensitiveInfoScanner()

# Scan a single file
result = scanner.scan_file('/path/to/config.env')

# Scan multiple files
files = ['/path/file1.txt', '/path/file2.py']
results = scanner.scan_files(files)

# Scan entire directory
results = scanner.scan_directory('/path/to/project', recursive=True)

# Scan with file patterns
results = scanner.scan_directory(
    '/path/to/project',
    recursive=True,
    file_patterns=['*.env', '*.config', '*.yaml']
)
```

### Using External Pattern Providers

```python
from discover.sensitive_scanner_enhanced import (
    EnhancedSensitiveInfoScanner,
    SensitivePatterns,
    ExternalPatternProvider
)

# Load patterns from JSON file
external_provider = ExternalPatternProvider(
    source_file='/path/to/patterns.json',
    cache_ttl=3600  # Cache for 1 hour
)

# Load patterns from URL
external_provider = ExternalPatternProvider(
    source_url='https://example.com/patterns.json',
    cache_ttl=3600
)

# Use multiple providers
scanner = EnhancedSensitiveInfoScanner(
    pattern_providers=[
        SensitivePatterns(),
        external_provider
    ]
)
```

### Custom Configuration

```python
from discover.sensitive_scanner_enhanced import EnhancedSensitiveInfoScanner

scanner = EnhancedSensitiveInfoScanner(
    timeout=15,              # Request timeout
    max_workers=10,          # Concurrent workers
    enable_heuristics=True,  # Enable heuristic detection
    enable_ml=False,         # Disable ML (template)
    cache_ttl=1800,          # 30-minute cache
    log_level='DEBUG'        # Detailed logging
)
```

### Heuristic Detection Only

```python
from discover.sensitive_scanner_enhanced import HeuristicScanner

content = "..."  # Your content to scan

# Detect high entropy strings
findings = HeuristicScanner.detect_high_entropy_strings(
    content,
    min_length=20,
    entropy_threshold=4.5
)

# Detect suspicious assignments
findings = HeuristicScanner.detect_suspicious_assignments(content)

# Calculate entropy
entropy = HeuristicScanner.calculate_entropy("some_string")
```

### Enhanced Function for Discovered URLs

```python
from discover.sensitive_scanner_enhanced import scan_discovered_urls_enhanced

urls = ['https://example1.com', 'https://example2.com']

result = scan_discovered_urls_enhanced(
    urls,
    max_urls=50,
    enable_heuristics=True,
    log_level='INFO'
)

# Access findings by severity
critical_findings = result['findings_by_severity']['critical']
high_findings = result['findings_by_severity']['high']

# Access findings by type
aws_keys = result['findings_by_type'].get('AWS Access Key', [])
```

### Custom Pattern Provider

```python
from discover.sensitive_scanner_enhanced import PatternProvider

class CustomPatternProvider(PatternProvider):
    def get_patterns(self):
        return {
            'My Custom Pattern': r'custom_\w+',
            'Another Pattern': r'special_[0-9]+'
        }
    
    def get_pattern_severity(self, pattern_name):
        return 'high' if 'Custom' in pattern_name else 'medium'

# Use custom provider
scanner = EnhancedSensitiveInfoScanner(
    pattern_providers=[CustomPatternProvider()]
)
```

### ML Integration (Template Example)

```python
from discover.sensitive_scanner_enhanced import MLIntegrationTemplate

# Extend the template
class MyMLDetector(MLIntegrationTemplate):
    def _load_model(self):
        # Example: Load sklearn model
        import joblib
        self.model = joblib.load(self.model_path)
    
    def predict_sensitive(self, text):
        if self.model is None:
            return False, 0.0
        
        features = self._extract_features(text)
        prob = self.model.predict_proba([features])[0][1]
        return prob > 0.7, prob
    
    def _extract_features(self, text):
        # Implement feature extraction
        return [
            len(text),
            HeuristicScanner.calculate_entropy(text),
            # ... more features
        ]

# Use in scanner
scanner = EnhancedSensitiveInfoScanner(
    enable_ml=True,
    ml_model_path='/path/to/model.pkl'
)
```

## External Pattern Configuration

Create a JSON file with the following structure:

```json
{
  "patterns": {
    "Pattern Name": "regex_pattern",
    "Another Pattern": "another_regex"
  },
  "severity": {
    "Pattern Name": "critical",
    "Another Pattern": "high"
  }
}
```

Severity levels: `critical`, `high`, `medium`, `low`

Example: See `external_patterns_example.json`

## Finding Structure

Each finding contains:

```python
{
    'type': 'Pattern Name',           # Type of sensitive data
    'value': 'actual_value',          # The sensitive value found
    'context': '...surrounding...',   # Context around the finding
    'position': 123,                  # Character position in content
    'source': 'url_or_file_path',    # Where it was found
    'source_type': 'url' or 'file',  # Type of source
    'severity': 'critical/high/...',  # Severity level
    'detection_method': 'pattern/heuristic/ml',  # How it was found
    
    # Optional fields
    'entropy': 4.8,                   # For heuristic findings
    'file_context': {...},            # For file scans
    'confidence': 0.85                # For ML findings
}
```

## Result Structure

```python
{
    'success': True,
    'total_urls_scanned': 10,
    'total_urls_failed': 0,
    'total_findings': 25,
    'findings_by_type': {
        'AWS Access Key': [...],
        'GitHub Token': [...]
    },
    'findings_by_severity': {
        'critical': [...],
        'high': [...],
        'medium': [...],
        'low': [...]
    },
    'all_findings': [...],
    'scan_results': [...]
}
```

## Performance Tuning

### Thread Pool Size
- **Low concurrency** (1-3 workers): For rate-limited APIs or low bandwidth
- **Medium concurrency** (5-10 workers): Default, balanced approach
- **High concurrency** (15-20 workers): For many small files or fast networks

### Cache TTL
- **Short TTL** (300-600s): For frequently changing content
- **Medium TTL** (1800-3600s): Default, balanced approach
- **Long TTL** (7200-14400s): For static content or development

### Heuristics
- **Enable** for comprehensive scanning and unknown pattern detection
- **Disable** for faster scanning with only known patterns

## Backward Compatibility

The original `sensitive_scanner.py` remains unchanged and fully functional. Enhanced features are available through:

```python
# Original (still works)
from discover.sensitive_scanner import SensitiveInfoScanner, scan_discovered_urls

# Enhanced (new features)
from discover.sensitive_scanner_enhanced import EnhancedSensitiveInfoScanner, scan_discovered_urls_enhanced
```

## Security Considerations

1. **SSL Verification**: Disabled by default for security testing. Enable in production.
2. **Sensitive Data Handling**: Findings contain actual sensitive values. Handle carefully.
3. **External Patterns**: Validate external pattern sources before use.
4. **Cache Security**: Cache may contain sensitive findings. Clear appropriately.
5. **Logging**: Be cautious with DEBUG level in production (may log sensitive data).

## Testing

Run tests with:

```bash
python -m pytest discover/test_sensitive_scanner_enhanced.py -v
```

Or:

```bash
python discover/test_sensitive_scanner_enhanced.py
```

## Migration Guide

### From Original Scanner

```python
# Old code
from discover.sensitive_scanner import SensitiveInfoScanner
scanner = SensitiveInfoScanner(timeout=10)
results = scanner.scan_urls(urls, max_workers=5)

# New code (drop-in replacement with more features)
from discover.sensitive_scanner_enhanced import EnhancedSensitiveInfoScanner
scanner = EnhancedSensitiveInfoScanner(timeout=10, max_workers=5)
results = scanner.scan_urls(urls)
```

### From scan_discovered_urls

```python
# Old code
from discover.sensitive_scanner import scan_discovered_urls
results = scan_discovered_urls(urls, max_urls=50)

# New code (drop-in with severity classification)
from discover.sensitive_scanner_enhanced import scan_discovered_urls_enhanced
results = scan_discovered_urls_enhanced(urls, max_urls=50)
# Now results include 'findings_by_severity' field
```

## Extending the Scanner

### Adding New Pattern Providers

1. Inherit from `PatternProvider`
2. Implement `get_patterns()` and `get_pattern_severity()`
3. Pass to scanner initialization

### Adding New Heuristics

1. Add static methods to `HeuristicScanner`
2. Call from `scan_content_for_sensitive_data()`
3. Mark findings with `detection_method='heuristic'`

### Integrating ML Models

1. Extend `MLIntegrationTemplate`
2. Implement `_load_model()`, `predict_sensitive()`, `_extract_features()`
3. Enable ML in scanner: `enable_ml=True, ml_model_path='...'`

## Troubleshooting

### High False Positive Rate
- Increase entropy threshold in heuristics
- Add context validation
- Use more specific patterns

### Low Detection Rate
- Enable heuristics
- Add custom patterns via external provider
- Lower entropy threshold

### Performance Issues
- Reduce max_workers
- Increase cache_ttl
- Disable heuristics for faster scanning
- Use file_patterns to scan specific files only

### Memory Usage
- Limit max_urls or file count
- Reduce max_workers
- Clear cache periodically: `scanner.cache.clear()`

## Contributing

To add new patterns to the built-in provider:

1. Add pattern constant to `SensitivePatterns`
2. Add to `get_patterns()` method
3. Add severity to `SEVERITY_MAP`
4. Add tests to `test_sensitive_scanner_enhanced.py`

## Support

For issues, questions, or contributions, refer to the main repository documentation.
